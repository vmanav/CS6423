{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VhIDIfICoS48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * 4:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * 4, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * 4)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out)) + self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], 1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], 2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], 2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], 2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_planes, planes, s))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
        "        x = F.avg_pool2d(x, 4).view(x.size(0), -1)\n",
        "        return self.linear(x)\n",
        "\n",
        "def ResNet101(num_classes): return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
        "\n",
        "class MobileNetV2_Block(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super().__init__()\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, out_planes, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_planes)\n",
        "            )\n",
        "        self.stride = stride\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.stride == 1:\n",
        "            out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    cfg = [(1, 16, 1, 1), (6, 24, 2, 1), (6, 32, 3, 2),\n",
        "           (6, 64, 4, 2), (6, 96, 3, 1), (6, 160, 3, 2), (6, 320, 1, 1)]\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(32)\n",
        "        self.conv2 = nn.Conv2d(320, 1280, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(1280)\n",
        "        self.linear = nn.Linear(1280, num_classes)\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for e, o, n, s in self.cfg:\n",
        "            for i in range(n):\n",
        "                layers.append(MobileNetV2_Block(in_planes, o, e, s if i == 0 else 1))\n",
        "                in_planes = o\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layers(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.avg_pool2d(x, 4).view(x.size(0), -1)\n",
        "        return self.linear(x)\n",
        "\n",
        "vgg_cfg = {\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M',\n",
        "              512, 512, 512, 'M', 512, 512, 512, 'M']\n",
        "}\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = self._make_layers(vgg_cfg[\"VGG16\"])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x).view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []; in_c = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M': layers += [nn.MaxPool2d(2,2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_c, x, 3, 1, 1), nn.BatchNorm2d(x), nn.ReLU()]\n",
        "                in_c = x\n",
        "        layers += [nn.AvgPool2d(1)]\n",
        "        return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "iJLQmbR5osHx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class pruning_engine_base:\n",
        "    def __init__(self, pruning_ratio, pruning_method):\n",
        "        self.pruning_ratio = 1 - pruning_ratio\n",
        "        self.pruning_method = pruning_method\n",
        "        self.mask_number = 0.0\n",
        "        self.device = device\n",
        "\n",
        "    def base_remove_filter_by_index(self, weight, remove_filter_idx, bias=None, mean=None, var=None, linear=False):\n",
        "        with torch.no_grad():\n",
        "            if mean is not None:\n",
        "                for idx in remove_filter_idx:\n",
        "                    weight[idx.item()] = self.mask_number\n",
        "                    bias[idx.item()] = self.mask_number\n",
        "                    mean[idx.item()] = self.mask_number\n",
        "                    var[idx.item()] = self.mask_number\n",
        "                return weight, bias, mean, var\n",
        "            elif bias is not None:\n",
        "                for idx in remove_filter_idx:\n",
        "                    weight[idx.item()] = self.mask_number\n",
        "                    bias[idx.item()] = self.mask_number\n",
        "                return weight, bias\n",
        "            else:\n",
        "                for idx in remove_filter_idx:\n",
        "                    weight[idx.item()] = self.mask_number\n",
        "                return weight\n",
        "\n",
        "    def base_remove_kernel_by_index(self, weight, remove_filter_idx, linear=False):\n",
        "        with torch.no_grad():\n",
        "            for idx in remove_filter_idx:\n",
        "                weight[:, idx.item()] = self.mask_number\n",
        "        return weight\n",
        "\n",
        "\n",
        "class Taylor:\n",
        "    def __init__(self, tool_net, taylor_loader, total_layer, total_sample_size):\n",
        "        self.tool_net = tool_net\n",
        "        self.taylor_loader = taylor_loader\n",
        "        self.total_layer = total_layer\n",
        "        self.total_sample_size = total_sample_size\n",
        "        self.forward_activations = [None] * total_layer\n",
        "        self.backward_gradients = [None] * total_layer\n",
        "\n",
        "    def attach_hooks(self, conv_layers):\n",
        "        def make_forward_hook(i):\n",
        "            def forward_hook(module, inp, out):\n",
        "                with torch.no_grad():\n",
        "                    abs_out = out.detach().abs().sum(dim=0)\n",
        "                    if self.forward_activations[i] is None:\n",
        "                        self.forward_activations[i] = abs_out\n",
        "                    else:\n",
        "                        if self.forward_activations[i].shape != abs_out.shape:\n",
        "                            self.forward_activations[i] = abs_out\n",
        "                        else:\n",
        "                            self.forward_activations[i] += abs_out\n",
        "            return forward_hook\n",
        "\n",
        "        def make_backward_hook(i):\n",
        "            def backward_hook(module, grad_in, grad_out):\n",
        "                with torch.no_grad():\n",
        "                    grad = grad_out[0].abs().sum(dim=0)\n",
        "                    if self.backward_gradients[i] is None:\n",
        "                        self.backward_gradients[i] = grad\n",
        "                    else:\n",
        "                        if self.backward_gradients[i].shape != grad.shape:\n",
        "                            self.backward_gradients[i] = grad\n",
        "                        else:\n",
        "                            self.backward_gradients[i] += grad\n",
        "            return backward_hook\n",
        "\n",
        "        for i, conv in enumerate(conv_layers):\n",
        "            conv.register_forward_hook(make_forward_hook(i))\n",
        "            conv.register_backward_hook(make_backward_hook(i))\n",
        "\n",
        "    def Taylor_add_gradient(self):\n",
        "        conv_layers = [m for m in self.tool_net.modules() if isinstance(m, nn.Conv2d)]\n",
        "        assert len(conv_layers) == self.total_layer, \"Mismatch in layer count.\"\n",
        "        self.attach_hooks(conv_layers)\n",
        "        optimizer = optim.Adam(self.tool_net.parameters())\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        self.tool_net.train()\n",
        "        with tqdm(total=len(self.taylor_loader)) as pbar:\n",
        "            for inputs, labels in self.taylor_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.tool_net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                pbar.update()\n",
        "        for i in range(self.total_layer):\n",
        "            if self.forward_activations[i] is not None:\n",
        "                self.forward_activations[i] /= float(self.total_sample_size)\n",
        "            if self.backward_gradients[i] is not None:\n",
        "                self.backward_gradients[i] /= float(self.total_sample_size)\n",
        "\n",
        "    def store_grad_layer(self, layer_store):\n",
        "        for i, layer in enumerate(layer_store):\n",
        "            layer.__dict__[\"feature_map\"] = self.forward_activations[i]\n",
        "            layer.__dict__[\"gradient\"] = self.backward_gradients[i]\n",
        "\n",
        "    def clear_stats(self):\n",
        "        self.forward_activations = [None] * self.total_layer\n",
        "        self.backward_gradients = [None] * self.total_layer\n",
        "\n",
        "    def Taylor_pruning(self, layer):\n",
        "        fm = layer.feature_map\n",
        "        gr = layer.gradient\n",
        "        importance = (fm * gr).abs().mean(dim=(1, 2))\n",
        "        importance /= (importance.norm() + 1e-8)\n",
        "        _, sorted_idx = torch.sort(importance, descending=True)\n",
        "        return sorted_idx\n",
        "\n",
        "\n",
        "class pruning_engine(pruning_engine_base):\n",
        "    def __init__(self, pruning_method, pruning_ratio=0.0, individual=False,\n",
        "                 conv_to_bn_map=None, **kwargs):\n",
        "        super().__init__(pruning_ratio, pruning_method)\n",
        "        self.conv_to_bn_map = conv_to_bn_map if conv_to_bn_map else {}\n",
        "        self.remove_filter_idx_history = {\"previous_layer\": None, \"current_layer\": None}\n",
        "        self.individual = individual\n",
        "        if self.pruning_method == \"Taylor\":\n",
        "            layer_store = kwargs[\"layer_store_private_variable\"]\n",
        "            self.taylor_helper = Taylor(\n",
        "                tool_net=kwargs[\"tool_net\"],\n",
        "                taylor_loader=kwargs[\"taylor_loader\"],\n",
        "                total_layer=len(layer_store),\n",
        "                total_sample_size=kwargs[\"total_sample_size\"]\n",
        "            )\n",
        "            self.taylor_helper.clear_stats()\n",
        "            self.taylor_helper.Taylor_add_gradient()\n",
        "            self.taylor_helper.store_grad_layer(layer_store)\n",
        "            self.pruning_criterion = self.taylor_helper.Taylor_pruning\n",
        "\n",
        "    def set_layer(self, layer, main_layer=False):\n",
        "        self.copy_layer = deepcopy(layer)\n",
        "        if main_layer:\n",
        "            if self.individual:\n",
        "                self.remove_filter_idx_history = {\"previous_layer\": None, \"current_layer\": None}\n",
        "            self.remove_filter_idx_history[\"previous_layer\"] = self.remove_filter_idx_history[\"current_layer\"]\n",
        "            self.remove_filter_idx_history[\"current_layer\"] = None\n",
        "            remove_filter_idx = self.pruning_criterion(self.copy_layer)\n",
        "            num_prune = int(len(remove_filter_idx) * self.pruning_ratio)\n",
        "            self.remove_filter_idx = remove_filter_idx[num_prune:]\n",
        "            if self.remove_filter_idx_history[\"previous_layer\"] is None:\n",
        "                self.remove_filter_idx_history[\"previous_layer\"] = self.remove_filter_idx\n",
        "            self.remove_filter_idx_history[\"current_layer\"] = self.remove_filter_idx\n",
        "        return True\n",
        "\n",
        "    def remove_conv_filter_kernel(self, conv_name=None, model=None):\n",
        "        if self.copy_layer.bias is not None:\n",
        "            w, b = self.base_remove_filter_by_index(\n",
        "                weight=self.copy_layer.weight.data,\n",
        "                remove_filter_idx=self.remove_filter_idx_history[\"current_layer\"],\n",
        "                bias=self.copy_layer.bias.data\n",
        "            )\n",
        "            self.copy_layer.weight.data = w\n",
        "            self.copy_layer.bias.data = b\n",
        "        else:\n",
        "            w = self.base_remove_filter_by_index(\n",
        "                weight=self.copy_layer.weight.data,\n",
        "                remove_filter_idx=self.remove_filter_idx_history[\"current_layer\"]\n",
        "            )\n",
        "            self.copy_layer.weight.data = w\n",
        "        if conv_name and model and (conv_name in self.conv_to_bn_map):\n",
        "            bn_name = self.conv_to_bn_map[conv_name]\n",
        "            bn_layer = get_module_by_name(model, bn_name)\n",
        "            self.remove_bn_layer(bn_layer, self.remove_filter_idx_history[\"current_layer\"])\n",
        "        return self.copy_layer\n",
        "\n",
        "    def remove_bn_layer(self, bn_layer, remove_filter_idx):\n",
        "        w, b, m, v = self.base_remove_filter_by_index(\n",
        "            weight=bn_layer.weight.data,\n",
        "            remove_filter_idx=remove_filter_idx,\n",
        "            bias=bn_layer.bias.data,\n",
        "            mean=bn_layer.running_mean.data,\n",
        "            var=bn_layer.running_var.data\n",
        "        )\n",
        "        bn_layer.weight.data = w\n",
        "        bn_layer.bias.data = b\n",
        "        bn_layer.running_mean.data = m\n",
        "        bn_layer.running_var.data = v\n"
      ],
      "metadata": {
        "id": "Olx5CuRsqwb3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaylorHook:\n",
        "    def __init__(self, model, loader, total_layers, total_samples):\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.total_layers = total_layers\n",
        "        self.total_samples = total_samples\n",
        "        self.f_acts = [None] * total_layers\n",
        "        self.b_grads = [None] * total_layers\n",
        "\n",
        "    def attach_hooks(self, conv_layers):\n",
        "        def fwd_hook(i):\n",
        "            def hook(module, inp, out):\n",
        "                val = out.detach().abs().sum(dim=0)\n",
        "                self.f_acts[i] = val if self.f_acts[i] is None else self.f_acts[i] + val\n",
        "            return hook\n",
        "        def bwd_hook(i):\n",
        "            def hook(module, grad_in, grad_out):\n",
        "                val = grad_out[0].abs().sum(dim=0)\n",
        "                self.b_grads[i] = val if self.b_grads[i] is None else self.b_grads[i] + val\n",
        "            return hook\n",
        "        for i, conv in enumerate(conv_layers):\n",
        "            conv.register_forward_hook(fwd_hook(i))\n",
        "            conv.register_backward_hook(bwd_hook(i))\n",
        "\n",
        "    def compute(self):\n",
        "        layers = [m for m in self.model.modules() if isinstance(m, nn.Conv2d)]\n",
        "        assert len(layers) == self.total_layers\n",
        "        self.attach_hooks(layers)\n",
        "        opt = optim.Adam(self.model.parameters())\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        self.model.train()\n",
        "        for x, y in tqdm(self.loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = self.model(x)\n",
        "            loss_fn(out, y).backward()\n",
        "        for i in range(self.total_layers):\n",
        "            self.f_acts[i] /= self.total_samples\n",
        "            self.b_grads[i] /= self.total_samples\n",
        "\n",
        "    def get_importance(self, conv_layer, idx):\n",
        "        fm = self.f_acts[idx]\n",
        "        gr = self.b_grads[idx]\n",
        "        imp = (fm * gr).abs().mean(dim=(1,2))\n",
        "        normed = imp / (imp.norm() + 1e-8)\n",
        "        return torch.argsort(normed, descending=True)\n"
      ],
      "metadata": {
        "id": "513W-S-Ho1xw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_module_by_name(model, access_string):\n",
        "    for name in access_string.split(\".\"):\n",
        "        model = getattr(model, name)\n",
        "    return model\n",
        "\n",
        "def set_module_by_name(model, access_string, new_module):\n",
        "    names = access_string.split(\".\")\n",
        "    for name in names[:-1]:\n",
        "        model = getattr(model, name)\n",
        "    setattr(model, names[-1], new_module)\n",
        "\n",
        "def count_nonzero_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
        "    compression_ratio = total_params / nonzero_params if nonzero_params else float('inf')\n",
        "    sparsity = (1 - (nonzero_params / total_params)) * 100\n",
        "    print(f\"Total: {total_params}, Nonzero: {nonzero_params}, Compression: {compression_ratio:.2f}x, Sparsity: {sparsity:.2f}%\")\n",
        "    return compression_ratio, sparsity\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = top1 = top5 = total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            _, pred = out.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += pred.eq(y).sum().item()\n",
        "            _, topk = out.topk(5, 1, True, True)\n",
        "            top1 += topk[:,0].eq(y).sum().item()\n",
        "            top5 += torch.any(topk.eq(y.view(-1, 1)), dim=1).sum().item()\n",
        "            all_preds += pred.cpu().tolist()\n",
        "            all_labels += y.cpu().tolist()\n",
        "    print(f\"Acc: {100*correct/total:.2f}, Top1: {100*top1/total:.2f}, Top5: {100*top5/total:.2f}\")\n",
        "    print(f\"Prec: {precision_score(all_labels, all_preds, average='macro'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(all_labels, all_preds, average='macro'):.4f}\")\n",
        "    print(f\"F1: {f1_score(all_labels, all_preds, average='macro'):.4f}\")\n",
        "    return all_preds, all_labels\n",
        "\n",
        "def measure_inference_speed(model, dataloader, device, num_batches=10):\n",
        "    model.eval(); total_time = 0.0; num_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(dataloader):\n",
        "            if i >= num_batches: break\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = time.time()\n",
        "            _ = model(inputs)\n",
        "            end_time = time.time()\n",
        "            total_time += end_time - start_time\n",
        "            num_samples += inputs.size(0)\n",
        "    print(f\"Infer Speed: {total_time/num_samples:.6f} sec/sample ({num_samples/total_time:.2f} FPS)\")\n"
      ],
      "metadata": {
        "id": "KR3QfLmZpFGO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(name):\n",
        "    if name == \"cifar10\":\n",
        "        norm = ((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)); num_classes = 10\n",
        "        Dataset = torchvision.datasets.CIFAR10\n",
        "    else:\n",
        "        norm = ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)); num_classes = 100\n",
        "        Dataset = torchvision.datasets.CIFAR100\n",
        "    tf_train = transforms.Compose([transforms.RandomCrop(32, 4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(*norm)])\n",
        "    tf_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*norm)])\n",
        "    trainset = Dataset(root=\"./data\", train=True, download=True, transform=tf_train)\n",
        "    testset = Dataset(root=\"./data\", train=False, download=True, transform=tf_test)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    return trainloader, testloader, trainloader, num_classes\n"
      ],
      "metadata": {
        "id": "M-yi8gn9pIOE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, criterion, epochs=2):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(dataloader):.4f} Time: {time.time()-start_time:.2f}s\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "PSMLqrKnrt2M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_pipeline(model_name, dataset_name, pretrained_path, one_shot_ratio=0.2, iter_ratio=0.1, iter_steps=2):\n",
        "    print(f\"\\n=== {model_name.upper()} on {dataset_name.upper()} ===\")\n",
        "    trainloader, testloader, taylor_loader, num_classes = get_dataset(dataset_name)\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        model = ResNet101(num_classes)\n",
        "    elif model_name == \"mobilenet\":\n",
        "        model = MobileNetV2(num_classes)\n",
        "    else:\n",
        "        model = VGG(num_classes)\n",
        "\n",
        "    checkpoint = torch.load(pretrained_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint, strict=False)\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"\\n-- Baseline --\")\n",
        "    evaluate_model(model, testloader)\n",
        "    count_nonzero_params(model)\n",
        "    measure_inference_speed(model, testloader, device)\n",
        "\n",
        "    layer_store = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    conv_to_bn_map = {}\n",
        "    prev_conv = None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d): prev_conv = name\n",
        "        elif isinstance(module, nn.BatchNorm2d) and prev_conv:\n",
        "            conv_to_bn_map[prev_conv] = name\n",
        "            prev_conv = None\n",
        "\n",
        "    print(\"\\n-- One-Shot Pruning --\")\n",
        "    pe = pruning_engine(\n",
        "        pruning_method=\"Taylor\",\n",
        "        pruning_ratio=one_shot_ratio,\n",
        "        tool_net=model,\n",
        "        taylor_loader=taylor_loader,\n",
        "        total_sample_size=len(trainloader.dataset),\n",
        "        layer_store_private_variable=layer_store,\n",
        "        conv_to_bn_map=conv_to_bn_map\n",
        "    )\n",
        "\n",
        "    for lname in conv_to_bn_map:\n",
        "        orig_layer = get_module_by_name(model, lname)\n",
        "        pe.set_layer(orig_layer, main_layer=True)\n",
        "        masked_layer = pe.remove_conv_filter_kernel(conv_name=lname, model=model)\n",
        "        set_module_by_name(model, lname, masked_layer)\n",
        "\n",
        "    evaluate_model(model, testloader)\n",
        "    count_nonzero_params(model)\n",
        "    measure_inference_speed(model, testloader, device)\n",
        "\n",
        "    print(\"\\n-- Iterative Pruning --\")\n",
        "    for it in range(iter_steps):\n",
        "        print(f\"Iter {it+1}\")\n",
        "        pe_iter = pruning_engine(\n",
        "            pruning_method=\"Taylor\",\n",
        "            pruning_ratio=iter_ratio,\n",
        "            tool_net=model,\n",
        "            taylor_loader=taylor_loader,\n",
        "            total_sample_size=len(trainloader.dataset),\n",
        "            layer_store_private_variable=[m for m in model.modules() if isinstance(m, nn.Conv2d)],\n",
        "            conv_to_bn_map=conv_to_bn_map\n",
        "        )\n",
        "        for lname in conv_to_bn_map:\n",
        "            current_layer = get_module_by_name(model, lname)\n",
        "            pe_iter.set_layer(current_layer, main_layer=True)\n",
        "            masked_layer = pe_iter.remove_conv_filter_kernel(conv_name=lname, model=model)\n",
        "            set_module_by_name(model, lname, masked_layer)\n",
        "        train_model(model, trainloader, optim.Adam(model.parameters()), nn.CrossEntropyLoss(), epochs=2)\n",
        "        evaluate_model(model, testloader)\n",
        "        count_nonzero_params(model)\n",
        "        measure_inference_speed(model, testloader, device)\n"
      ],
      "metadata": {
        "id": "nbAhcBu4pIT4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSDQaZlfqPe0",
        "outputId": "66e22353-7d5b-4195-c01c-617bf448ebfe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = {\n",
        "    \"resnet_cifar100\": \"/content/drive/MyDrive/Models/Model@ResNet101_CIFAR100.pt\",\n",
        "    \"mobilenet_cifar100\": \"/content/drive/MyDrive/Models/Model@Mobilenetv2_CIFAR100.pt\",\n",
        "    \"vgg_cifar100\": \"/content/drive/MyDrive/Models/Model@VGG16_CIFAR100.pt\",\n",
        "    \"mobilenet_cifar10\": \"/content/drive/MyDrive/Models/Model@Mobilenetv2_CIFAR10.pt\",\n",
        "    \"vgg_cifar10\": \"/content/drive/MyDrive/Models/Model@VGG16_CIFAR10.pt\"\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "b8tpRfMMpSn_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_pipeline(\"mobilenet\", \"cifar10\", model_paths[\"mobilenet_cifar10\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeaq_6dswbRH",
        "outputId": "d297ba26-4562-46de-a31f-3e2151c1204d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MOBILENET on CIFAR10 ===\n",
            "\n",
            "-- Baseline --\n",
            "Acc: 95.80, Top1: 95.80, Top5: 99.78\n",
            "Prec: 0.9580\n",
            "Recall: 0.9580\n",
            "F1: 0.9579\n",
            "Total: 2296922, Nonzero: 2296922, Compression: 1.00x, Sparsity: 0.00%\n",
            "Infer Speed: 0.000229 sec/sample (4376.18 FPS)\n",
            "\n",
            "-- One-Shot Pruning --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:55<00:00,  7.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 21.98, Top1: 21.98, Top5: 68.35\n",
            "Prec: 0.6938\n",
            "Recall: 0.2198\n",
            "F1: 0.1606\n",
            "Total: 2296922, Nonzero: 1837629, Compression: 1.25x, Sparsity: 20.00%\n",
            "Infer Speed: 0.000258 sec/sample (3874.19 FPS)\n",
            "\n",
            "-- Iterative Pruning --\n",
            "Iter 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [01:02<00:00,  6.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 0.4642 Time: 63.64s\n",
            "Epoch [2/2] Loss: 0.3707 Time: 63.75s\n",
            "Acc: 86.56, Top1: 86.56, Top5: 99.51\n",
            "Prec: 0.8692\n",
            "Recall: 0.8656\n",
            "F1: 0.8644\n",
            "Total: 2296922, Nonzero: 1807894, Compression: 1.27x, Sparsity: 21.29%\n",
            "Infer Speed: 0.000505 sec/sample (1979.41 FPS)\n",
            "Iter 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [01:10<00:00,  5.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 0.3607 Time: 71.68s\n",
            "Epoch [2/2] Loss: 0.3156 Time: 71.67s\n",
            "Acc: 88.88, Top1: 88.88, Top5: 99.74\n",
            "Prec: 0.8898\n",
            "Recall: 0.8888\n",
            "F1: 0.8887\n",
            "Total: 2296922, Nonzero: 1807894, Compression: 1.27x, Sparsity: 21.29%\n",
            "Infer Speed: 0.000238 sec/sample (4205.77 FPS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_pipeline(\"vgg\", \"cifar10\", model_paths[\"vgg_cifar10\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZFDKlhSwdbI",
        "outputId": "faf7df7a-e03c-4cdb-f7ff-96688b253cda"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== VGG on CIFAR10 ===\n",
            "\n",
            "-- Baseline --\n",
            "Acc: 95.43, Top1: 95.43, Top5: 99.85\n",
            "Prec: 0.9544\n",
            "Recall: 0.9543\n",
            "F1: 0.9542\n",
            "Total: 33646666, Nonzero: 33646666, Compression: 1.00x, Sparsity: 0.00%\n",
            "Infer Speed: 0.000076 sec/sample (13093.66 FPS)\n",
            "\n",
            "-- One-Shot Pruning --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:25<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 51.82, Top1: 51.82, Top5: 87.09\n",
            "Prec: 0.7430\n",
            "Recall: 0.5182\n",
            "F1: 0.5065\n",
            "Total: 33646666, Nonzero: 30681391, Compression: 1.10x, Sparsity: 8.81%\n",
            "Infer Speed: 0.000089 sec/sample (11222.22 FPS)\n",
            "\n",
            "-- Iterative Pruning --\n",
            "Iter 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:26<00:00, 14.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 0.9084 Time: 29.90s\n",
            "Epoch [2/2] Loss: 0.5672 Time: 29.91s\n",
            "Acc: 80.15, Top1: 80.15, Top5: 97.23\n",
            "Prec: 0.8247\n",
            "Recall: 0.8015\n",
            "F1: 0.8033\n",
            "Total: 33646666, Nonzero: 30681391, Compression: 1.10x, Sparsity: 8.81%\n",
            "Infer Speed: 0.000125 sec/sample (8031.40 FPS)\n",
            "Iter 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:27<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 0.5285 Time: 31.12s\n",
            "Epoch [2/2] Loss: 0.4372 Time: 31.15s\n",
            "Acc: 84.73, Top1: 84.73, Top5: 98.32\n",
            "Prec: 0.8580\n",
            "Recall: 0.8473\n",
            "F1: 0.8485\n",
            "Total: 33646666, Nonzero: 30681391, Compression: 1.10x, Sparsity: 8.81%\n",
            "Infer Speed: 0.000168 sec/sample (5963.51 FPS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_pipeline(\"resnet\", \"cifar100\", model_paths[\"resnet_cifar100\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2x0muOwfjA",
        "outputId": "104e852d-1d42-4de0-de16-02cd08487653"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RESNET on CIFAR100 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 41.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Baseline --\n",
            "Acc: 83.38, Top1: 83.38, Top5: 96.20\n",
            "Prec: 0.8353\n",
            "Recall: 0.8338\n",
            "F1: 0.8336\n",
            "Total: 42697380, Nonzero: 42697380, Compression: 1.00x, Sparsity: 0.00%\n",
            "Infer Speed: 0.000161 sec/sample (6203.07 FPS)\n",
            "\n",
            "-- One-Shot Pruning --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [04:35<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 10.16, Top1: 10.16, Top5: 30.56\n",
            "Prec: 0.4529\n",
            "Recall: 0.1016\n",
            "F1: 0.1084\n",
            "Total: 42697380, Nonzero: 34120581, Compression: 1.25x, Sparsity: 20.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer Speed: 0.000391 sec/sample (2555.17 FPS)\n",
            "\n",
            "-- Iterative Pruning --\n",
            "Iter 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [05:00<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 2.7295 Time: 306.19s\n",
            "Epoch [2/2] Loss: 1.9218 Time: 305.97s\n",
            "Acc: 47.99, Top1: 47.99, Top5: 80.22\n",
            "Prec: 0.5281\n",
            "Recall: 0.4799\n",
            "F1: 0.4706\n",
            "Total: 42697380, Nonzero: 34127040, Compression: 1.25x, Sparsity: 20.07%\n",
            "Infer Speed: 0.000350 sec/sample (2854.14 FPS)\n",
            "Iter 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [05:22<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 1.6798 Time: 328.57s\n",
            "Epoch [2/2] Loss: 1.4307 Time: 327.96s\n",
            "Acc: 58.42, Top1: 58.42, Top5: 87.04\n",
            "Prec: 0.6141\n",
            "Recall: 0.5842\n",
            "F1: 0.5779\n",
            "Total: 42697380, Nonzero: 34127040, Compression: 1.25x, Sparsity: 20.07%\n",
            "Infer Speed: 0.000938 sec/sample (1065.63 FPS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_pipeline(\"mobilenet\", \"cifar100\", model_paths[\"mobilenet_cifar100\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh2aPb44wlqo",
        "outputId": "3874855d-1c57-4295-ac08-4ba766140578"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MOBILENET on CIFAR100 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 35.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Baseline --\n",
            "Acc: 78.80, Top1: 78.80, Top5: 95.22\n",
            "Prec: 0.7899\n",
            "Recall: 0.7880\n",
            "F1: 0.7878\n",
            "Total: 2412212, Nonzero: 2412212, Compression: 1.00x, Sparsity: 0.00%\n",
            "Infer Speed: 0.000167 sec/sample (5998.20 FPS)\n",
            "\n",
            "-- One-Shot Pruning --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:54<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 1.70, Top1: 1.70, Top5: 8.47\n",
            "Prec: 0.0102\n",
            "Recall: 0.0170\n",
            "F1: 0.0041\n",
            "Total: 2412212, Nonzero: 1952919, Compression: 1.24x, Sparsity: 19.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer Speed: 0.000262 sec/sample (3815.29 FPS)\n",
            "\n",
            "-- Iterative Pruning --\n",
            "Iter 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [01:01<00:00,  6.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 1.4515 Time: 63.56s\n",
            "Epoch [2/2] Loss: 1.1689 Time: 63.51s\n",
            "Acc: 60.69, Top1: 60.69, Top5: 87.34\n",
            "Prec: 0.6545\n",
            "Recall: 0.6069\n",
            "F1: 0.6062\n",
            "Total: 2412212, Nonzero: 1925761, Compression: 1.25x, Sparsity: 20.17%\n",
            "Infer Speed: 0.000271 sec/sample (3694.24 FPS)\n",
            "Iter 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [01:10<00:00,  5.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 1.1311 Time: 71.86s\n",
            "Epoch [2/2] Loss: 1.0008 Time: 71.75s\n",
            "Acc: 63.66, Top1: 63.66, Top5: 89.43\n",
            "Prec: 0.6686\n",
            "Recall: 0.6366\n",
            "F1: 0.6363\n",
            "Total: 2412212, Nonzero: 1925761, Compression: 1.25x, Sparsity: 20.17%\n",
            "Infer Speed: 0.000414 sec/sample (2414.15 FPS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_pipeline(\"vgg\", \"cifar100\", model_paths[\"vgg_cifar100\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNdES7Wwo2j",
        "outputId": "ce2f8404-1a7b-4116-fb8f-2c18dffec42c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== VGG on CIFAR100 ===\n",
            "\n",
            "-- Baseline --\n",
            "Acc: 76.51, Top1: 76.51, Top5: 93.48\n",
            "Prec: 0.7680\n",
            "Recall: 0.7651\n",
            "F1: 0.7653\n",
            "Total: 34015396, Nonzero: 34015396, Compression: 1.00x, Sparsity: 0.00%\n",
            "Infer Speed: 0.000066 sec/sample (15048.29 FPS)\n",
            "\n",
            "-- One-Shot Pruning --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:25<00:00, 15.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 22.27, Top1: 22.27, Top5: 50.22\n",
            "Prec: 0.5350\n",
            "Recall: 0.2227\n",
            "F1: 0.2380\n",
            "Total: 34015396, Nonzero: 31050121, Compression: 1.10x, Sparsity: 8.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer Speed: 0.000078 sec/sample (12853.43 FPS)\n",
            "\n",
            "-- Iterative Pruning --\n",
            "Iter 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:26<00:00, 14.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 2.4196 Time: 29.95s\n",
            "Epoch [2/2] Loss: 1.9217 Time: 30.05s\n",
            "Acc: 46.88, Top1: 46.88, Top5: 76.85\n",
            "Prec: 0.5402\n",
            "Recall: 0.4688\n",
            "F1: 0.4560\n",
            "Total: 34015396, Nonzero: 31050121, Compression: 1.10x, Sparsity: 8.72%\n",
            "Infer Speed: 0.000117 sec/sample (8536.65 FPS)\n",
            "Iter 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "100%|██████████| 391/391 [00:27<00:00, 14.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] Loss: 1.8099 Time: 31.25s\n",
            "Epoch [2/2] Loss: 1.5851 Time: 31.18s\n",
            "Acc: 55.73, Top1: 55.73, Top5: 83.35\n",
            "Prec: 0.6122\n",
            "Recall: 0.5573\n",
            "F1: 0.5536\n",
            "Total: 34015396, Nonzero: 31050121, Compression: 1.10x, Sparsity: 8.72%\n",
            "Infer Speed: 0.000161 sec/sample (6196.05 FPS)\n"
          ]
        }
      ]
    }
  ]
}